{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab Group: Lianhe Chu, Jie Zhong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Random indexing with permutations\n",
    "    1. You have got the code to train word embedding using Random indexing.\n",
    "    2. The code also estimates the performance on TOEFL task\n",
    "    3. The current code works adequately only when size of window is 2. What\n",
    "    should be changed in order to overcome this issue?\n",
    "    4. Get the performance of the model for three different dimensionalities. The\n",
    "    choice of dimensionalities is on your own but use different values (e.g.,\n",
    "    1000, 4000, 10000).\n",
    "    5. If your computational resources allow run simulations several times (e.g.\n",
    "    5) for each dimensionality\n",
    "    6. Report the accuracy on TOEFL for all simulations\n",
    "    7. Elaborate how accuracy changes with the dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import text_functions as tf\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimension_study(dimensionnumber):\n",
    "    a = 0.0  # accuracy of the encodings\n",
    "    # Number of times to run the code\n",
    "    num_runs = 5\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        print(f\"\\nRun {run + 1}:\")\n",
    "\n",
    "\n",
    "        threshold = 15000 # Frequency threshold in the corpus ??\n",
    "        dimension = dimensionnumber # Dimensionality for high-dimensional vectors\n",
    "        lemmatizer = nltk.WordNetLemmatizer()  # create an instance of lemmatizer\n",
    "        ones_number = 2 # number of nonzero elements in randomly generated high-dimensional vectors\n",
    "        window_size = 2 #number of neighboring words to consider both back and forth. \n",
    "        #In other words number of words before/after current word\n",
    "        zero_vector = np.zeros(dimension)\n",
    "        test_name = \"new_toefl.txt\" # file with TOEFL dataset\n",
    "        data_file_name = \"lemmatized.text\" # file with the text corpus\n",
    "\n",
    "        amount_dictionary = {}\n",
    "\n",
    "        # Count how many times each word appears in the corpus\n",
    "        text_file = open(data_file_name, \"r\")\n",
    "        for line in text_file:\n",
    "            if line != \"\\n\":\n",
    "                words = line.split()\n",
    "                for word in words:\n",
    "                    if amount_dictionary.get(word) is None:\n",
    "                        amount_dictionary[word] = 1\n",
    "                    else:\n",
    "                        amount_dictionary[word] += 1\n",
    "        text_file.close()\n",
    "\n",
    "        dictionary = {} #vocabulary and corresponing random high-dimensional vectors\n",
    "        word_space = {} #embedings\n",
    "\n",
    "        #Create a dictionary with the assigned random high-dimensional vectors\n",
    "        text_file = open(data_file_name, \"r\")\n",
    "        for line in text_file: #read line in the file\n",
    "            words = line.split() # extract words from the line\n",
    "            for word in words:  # for each word\n",
    "                if dictionary.get(word) is None: # If the word was not yed added to the vocabulary\n",
    "                    if amount_dictionary[word] < threshold:\n",
    "                        dictionary[word] = tf.get_random_word_vector(dimension, ones_number) # assign a  \n",
    "                    else:\n",
    "                        dictionary[word] = np.zeros(dimension) # frequent words are assigned with empty vectors. In a way they will not contribute to the word embedding\n",
    "\n",
    "        text_file.close()\n",
    "\n",
    "\n",
    "        #Note that in order to save time we only create embeddings for the words needed in the TOEFL task\n",
    "\n",
    "            #Find all unique words amongst TOEFL tasks and initialize their embeddings to zeros    \n",
    "        number_of_tests = 0\n",
    "        text_file = open(test_name, \"r\") #open TOEFL tasks\n",
    "        for line in text_file:\n",
    "                words = line.split()\n",
    "                words = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word, 'v'), 'n'), 'a') for word in\n",
    "                        words] # lemmatize words in the current test\n",
    "                word_space[words[0]] = np.zeros(dimension)\n",
    "                word_space[words[1]] = np.zeros(dimension)\n",
    "                word_space[words[2]] = np.zeros(dimension)\n",
    "                word_space[words[3]] = np.zeros(dimension)\n",
    "                word_space[words[4]] = np.zeros(dimension)\n",
    "                number_of_tests += 1\n",
    "        text_file.close()\n",
    "\n",
    "        # Processing the text to build the embeddings \n",
    "        text_file = open(data_file_name, \"r\")\n",
    "        lines = [[],[],[],[]] # neighboring lines\n",
    "        i = 2\n",
    "        while i < 4:\n",
    "                line = \"\\n\"\n",
    "                while line == \"\\n\":\n",
    "                    line = text_file.readline()\n",
    "                lines[i] = line.split()\n",
    "                i += 1\n",
    "\n",
    "        line = text_file.readline()\n",
    "        while line != \"\":\n",
    "                if line != \"\\n\":\n",
    "                    lines.append(line.split())\n",
    "                    words = lines[2]\n",
    "                    length = len(words)\n",
    "                    i = 0\n",
    "                    while i < length:\n",
    "                        if not (word_space.get(words[i]) is None):\n",
    "                            k = 1\n",
    "                            word_space_vector = word_space[words[i]]\n",
    "                            while (i - k >= 0) and (k <= window_size): #process left neighbors of the focus word\n",
    "                                word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[words[i - k]], -1))         \n",
    "                                k += 1\n",
    "                            # Handle different situations if there was not enough neighbors on the left in the current line    \n",
    "                            if k <= window_size and (len(lines[1])>0): \n",
    "                                if len(lines[1]) < 2:\n",
    "                                    if k != 1: #if one word on the left was already added\n",
    "                                        word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[1][0]], -1)) #update word embedding\n",
    "                                    else:\n",
    "                                        word_space[words[i]] = np.add(word_space[words[i]],\n",
    "                                                                    np.roll(dictionary[lines[1][0]], -1)) #update word embedding\n",
    "                                        word_space[words[i]] = np.add(word_space[words[i]],\n",
    "                                                                    np.roll(dictionary[lines[0][len(lines[0]) - 1]], -1)) #update word embedding\n",
    "                                else:\n",
    "                                    if k != 1:\n",
    "                                        word_space[words[i]] = np.add(word_space[words[i]],\n",
    "                                                                    np.roll(dictionary[lines[1][len(lines[1]) - 1]], -1)) #update word embedding\n",
    "                                    else:\n",
    "                                        word_space[words[i]] = np.add(word_space[words[i]],\n",
    "                                                                    np.roll(dictionary[lines[1][len(lines[1]) - 1]], -1)) #update word embedding\n",
    "                                        word_space[words[i]] = np.add(word_space[words[i]],\n",
    "                                                                    np.roll(dictionary[lines[1][len(lines[1]) - 2]], -1)) #update word embedding\n",
    "\n",
    "                            k = 1\n",
    "                            while (i + k < length) and (k <= window_size): #process right neighbors of the focus word\n",
    "                                word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[words[i + k]], 1)) #update word embedding\n",
    "                                k += 1\n",
    "                            if k <= window_size:\n",
    "                                if len(lines[3]) < 2:\n",
    "                                    if k != 1:\n",
    "                                        word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[3][0]], 1)) #update word embedding\n",
    "                                    else:\n",
    "                                        word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[3][0]], 1)) #update word embedding\n",
    "                                        word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[4][0]], 1)) #update word embedding\n",
    "                                else:\n",
    "                                    if k != 1:\n",
    "                                        word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[3][0]], 1)) #update word embedding\n",
    "                                    else:\n",
    "                                        word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[3][0]], 1)) #update word embedding\n",
    "                                        word_space[words[i]] = np.add(word_space[words[i]],\n",
    "                                                                np.roll(dictionary[lines[3][1]], 1))\n",
    "\n",
    "                        i += 1\n",
    "                    lines.pop(0)\n",
    "                line = text_file.readline()\n",
    "\n",
    "        # Testing of the embeddings on TOEFL\n",
    "\n",
    "        i = 0\n",
    "        text_file = open(test_name, 'r')\n",
    "        right_answers = 0.0  # variable for correct answers\n",
    "        number_skipped_tests = 0.0  # some tests could be skipped if there are no corresponding words in the vocabulary extracted from the training corpus\n",
    "        while i < number_of_tests:\n",
    "            line = text_file.readline()  # read line in the file\n",
    "            words = line.split()  # extract words from the line\n",
    "            words = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word, 'v'), 'n'), 'a') for word in\n",
    "                    words]  # lemmatize words in the current test\n",
    "            try:\n",
    "\n",
    "                if not (amount_dictionary.get(words[0]) is None):  # check if there word in the corpus for the query word\n",
    "                    k = 1\n",
    "                    while k < 5:\n",
    "                        if np.array_equal(word_space[words[k]], zero_vector):  # if no representation was learned assign a random vector\n",
    "                            word_space[words[k]] = np.random.randn(dimension)\n",
    "                        k += 1\n",
    "                    right_answers += tf.get_answer_mod(\n",
    "                        [word_space[words[0]], word_space[words[1]], word_space[words[2]],\n",
    "                        word_space[words[3]], word_space[words[4]]])  # check if the word is predicted right\n",
    "            except KeyError:  # if there is no representation for the query vector then skip\n",
    "                number_skipped_tests += 1\n",
    "                print(\"skipped test: \" + str(i) + \"; Line: \" + str(words))\n",
    "            except IndexError:\n",
    "                print(i)\n",
    "                print(line)\n",
    "                print(words)\n",
    "                break\n",
    "            i += 1\n",
    "        text_file.close()\n",
    "        a += 100 * right_answers / number_of_tests\n",
    "        print(str(dimension) + \" Dimension percentage of correct answers: \" + str(100 * right_answers / number_of_tests) + \"%\")\n",
    "    print(\" average accuarcy: \" + str(a/num_runs) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop over function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 1:\n",
      "1000 Dimension percentage of correct answers: 73.75%\n",
      "\n",
      "Run 2:\n",
      "1000 Dimension percentage of correct answers: 67.5%\n",
      "\n",
      "Run 3:\n",
      "1000 Dimension percentage of correct answers: 72.5%\n",
      "\n",
      "Run 4:\n",
      "1000 Dimension percentage of correct answers: 70.0%\n",
      "\n",
      "Run 5:\n",
      "1000 Dimension percentage of correct answers: 68.75%\n",
      " average accuarcy: 70.5%\n",
      "\n",
      "Run 1:\n",
      "4000 Dimension percentage of correct answers: 72.5%\n",
      "\n",
      "Run 2:\n",
      "4000 Dimension percentage of correct answers: 71.25%\n",
      "\n",
      "Run 3:\n",
      "4000 Dimension percentage of correct answers: 72.5%\n",
      "\n",
      "Run 4:\n",
      "4000 Dimension percentage of correct answers: 72.5%\n",
      "\n",
      "Run 5:\n",
      "4000 Dimension percentage of correct answers: 71.25%\n",
      " average accuarcy: 72.0%\n",
      "\n",
      "Run 1:\n",
      "10000 Dimension percentage of correct answers: 75.0%\n",
      "\n",
      "Run 2:\n",
      "10000 Dimension percentage of correct answers: 75.0%\n",
      "\n",
      "Run 3:\n",
      "10000 Dimension percentage of correct answers: 70.0%\n",
      "\n",
      "Run 4:\n",
      "10000 Dimension percentage of correct answers: 75.0%\n",
      "\n",
      "Run 5:\n",
      "10000 Dimension percentage of correct answers: 73.75%\n",
      " average accuarcy: 73.75%\n"
     ]
    }
   ],
   "source": [
    "dimensionlist = [1000, 4000, 10000]\n",
    "for dimnr in dimensionlist:\n",
    "    dimension_study(dimnr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report the average accuracy on TOEFL for all simulations: \n",
    "* 1000 dimension: 70.5%\n",
    "* 4000 dimension: 72.0%%\n",
    "* 10000 dimension: 73.75%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elaborate how accuracy changes with the dimensionality:\n",
    "\n",
    "* Observation: the accuary increases with the increased dimensionality with the above study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
